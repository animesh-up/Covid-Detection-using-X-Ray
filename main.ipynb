{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c5332b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4f015",
   "metadata": {},
   "source": [
    "# Reading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76b98cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1269040109.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    directory = (\"C:\\Users\\Animesh\\Desktop\\ML\\Covid Detection using X-Ray\")\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "#* Reading main direcotry and creating dataframes for train and test data\n",
    "directory = (\"C:\\Users\\Animesh\\Desktop\\ML\\Covid Detection using X-Ray\")\n",
    "Train_data = pd.DataFrame(columns=['path', 'class'])\n",
    "Test_data = pd.DataFrame(columns=['path', 'class'])\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    for filename2 in os.listdir(directory+'/'+filename):\n",
    "        for images in os.listdir(directory+'/'+filename+'/'+filename2):\n",
    "            if filename =='train':\n",
    "                Train_data = Train_data.append({'path': directory+'/'+filename+'/'+filename2+'/'+images , 'class': filename2}, ignore_index=True)\n",
    "            else :\n",
    "                Test_data = Test_data.append({'path': directory+'/'+filename+'/'+filename2+'/'+images , 'class': filename2}, ignore_index=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3310e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [path, class]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d176dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Train_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c636b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [path, class]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498d2412",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 2 (2021218454.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    title = Test_data['class'][i]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "for i in range(462, 466):\n",
    "title = Test_data['class'][i]\n",
    "path = './X-RAY/test/COVID19/COVID19({}).jpg'\n",
    "image = mpimg.imread(path.format(i))\n",
    "plt.title('COVID19')\n",
    "plt.show()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217f53f",
   "metadata": {},
   "source": [
    "# Creating Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfce7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,GRU\n",
    "from keras.layers import Activation, TimeDistributed, LSTM, BatchNormalization \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#* Defining parameters\n",
    "batch_size = 24\n",
    "size = (224,224,3)\n",
    "img_width = img_hight = size[0]\n",
    "clases = ['COVID19', 'NORMAL', 'PNEUMONIA']\n",
    "\n",
    "#* Creating data generators\n",
    "data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05de2a45",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Animesh\\Desktop\\ML\\Covid Detection using X-Ray\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#* Creating train and validation data generators\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#* path -> from the dataframe\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#* x_col -> the column name that contains the path,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#* subset -> the subset of the data that will be used\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#* shuffle -> if the data will be shuffled or not\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_data \u001b[39m=\u001b[39m data_gen\u001b[39m.\u001b[39mflow_from_dataframe(Train_data, x_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m, y_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                               image_size\u001b[39m=\u001b[39m(img_hight, img_width), target_size\u001b[39m=\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                                                   img_hight, img_hight), color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                                               batch_size\u001b[39m=\u001b[39mbatch_size, class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                               classes\u001b[39m=\u001b[39mclases, subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m val_data \u001b[39m=\u001b[39m data_gen\u001b[39m.\u001b[39mflow_from_dataframe(Train_data, x_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m, y_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                                               image_size\u001b[39m=\u001b[39m(img_hight, img_width), target_size\u001b[39m=\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                                                   img_hight, img_hight), color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                                               batch_size\u001b[39m=\u001b[39mbatch_size, class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                               classes\u001b[39m=\u001b[39mclases, subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m test_data \u001b[39m=\u001b[39m data_gen\u001b[39m.\u001b[39mflow_from_dataframe(Test_data, x_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m, y_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                                               image_size\u001b[39m=\u001b[39m(img_hight, img_width), target_size\u001b[39m=\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                                                   img_hight, img_hight), color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                                               batch_size\u001b[39m=\u001b[39mbatch_size, class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Animesh/Desktop/ML/Covid%20Detection%20using%20X-Ray/main.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                               classes\u001b[39m=\u001b[39mclases, subset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:1807\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1800\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdrop_duplicates\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   1801\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1802\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1803\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1804\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1805\u001b[0m     )\n\u001b[1;32m-> 1807\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameIterator(\n\u001b[0;32m   1808\u001b[0m     dataframe,\n\u001b[0;32m   1809\u001b[0m     directory,\n\u001b[0;32m   1810\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1811\u001b[0m     x_col\u001b[39m=\u001b[39mx_col,\n\u001b[0;32m   1812\u001b[0m     y_col\u001b[39m=\u001b[39my_col,\n\u001b[0;32m   1813\u001b[0m     weight_col\u001b[39m=\u001b[39mweight_col,\n\u001b[0;32m   1814\u001b[0m     target_size\u001b[39m=\u001b[39mtarget_size,\n\u001b[0;32m   1815\u001b[0m     color_mode\u001b[39m=\u001b[39mcolor_mode,\n\u001b[0;32m   1816\u001b[0m     classes\u001b[39m=\u001b[39mclasses,\n\u001b[0;32m   1817\u001b[0m     class_mode\u001b[39m=\u001b[39mclass_mode,\n\u001b[0;32m   1818\u001b[0m     data_format\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_format,\n\u001b[0;32m   1819\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1820\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[0;32m   1821\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m   1822\u001b[0m     save_to_dir\u001b[39m=\u001b[39msave_to_dir,\n\u001b[0;32m   1823\u001b[0m     save_prefix\u001b[39m=\u001b[39msave_prefix,\n\u001b[0;32m   1824\u001b[0m     save_format\u001b[39m=\u001b[39msave_format,\n\u001b[0;32m   1825\u001b[0m     subset\u001b[39m=\u001b[39msubset,\n\u001b[0;32m   1826\u001b[0m     interpolation\u001b[39m=\u001b[39minterpolation,\n\u001b[0;32m   1827\u001b[0m     validate_filenames\u001b[39m=\u001b[39mvalidate_filenames,\n\u001b[0;32m   1828\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype,\n\u001b[0;32m   1829\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:974\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    972\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_valid_filepaths(df, x_col)\n\u001b[0;32m    973\u001b[0m \u001b[39mif\u001b[39;00m class_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulti_output\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m]:\n\u001b[1;32m--> 974\u001b[0m     df, classes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_classes(df, y_col, classes)\n\u001b[0;32m    975\u001b[0m     num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(classes)\n\u001b[0;32m    976\u001b[0m     \u001b[39m# build an index of all the unique classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:1108\u001b[0m, in \u001b[0;36mDataFrameIterator._filter_classes\u001b[1;34m(df, y_col, classes)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[39mif\u001b[39;00m classes:\n\u001b[0;32m   1106\u001b[0m     \u001b[39m# prepare for membership lookup\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(collections\u001b[39m.\u001b[39mOrderedDict\u001b[39m.\u001b[39mfromkeys(classes)\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m-> 1108\u001b[0m     df[y_col] \u001b[39m=\u001b[39m df[y_col]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: remove_classes(x, classes))\n\u001b[0;32m   1109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1110\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Animesh\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "#* Creating train and validation data generators\n",
    "#* path -> from the dataframe\n",
    "#* x_col -> the column name that contains the path,\n",
    "#* y_col -> the column name that contains the class\n",
    "#* image_size -> the size of the image\n",
    "#* target_size -> the size of the image that will be used for training\n",
    "#* color_mode -> the color mode of the image\n",
    "#* batch_size -> the size of the batch\n",
    "#* class_mode -> the type of the class\n",
    "#* classes -> the classes that will be used\n",
    "#* subset -> the subset of the data that will be used\n",
    "#* shuffle -> if the data will be shuffled or not\n",
    "\n",
    "train_data = data_gen.flow_from_dataframe(Train_data, x_col='path', y_col='class',\n",
    "                                              image_size=(img_hight, img_width), target_size=(\n",
    "                                                  img_hight, img_hight), color_mode='rgb',\n",
    "                                              batch_size=batch_size, class_mode='categorical',\n",
    "                                              classes=clases, subset='training')\n",
    "\n",
    "val_data = data_gen.flow_from_dataframe(Train_data, x_col='path', y_col='class',\n",
    "                                              image_size=(img_hight, img_width), target_size=(\n",
    "                                                  img_hight, img_hight), color_mode='rgb',\n",
    "                                              batch_size=batch_size, class_mode='categorical',\n",
    "                                              classes=clases, subset='validation')\n",
    "test_data = data_gen.flow_from_dataframe(Test_data, x_col='path', y_col='class',\n",
    "                                              image_size=(img_hight, img_width), target_size=(\n",
    "                                                  img_hight, img_hight), color_mode='rgb',\n",
    "                                              batch_size=batch_size, class_mode='categorical',\n",
    "                                              classes=clases, subset=None)\n",
    "\n",
    "###################################UnSHuffled#########################################\n",
    "#* will be used for the confusion matrix analysis for results \n",
    " \n",
    "test_data_unshuffled = data_gen.flow_from_dataframe(Test_data, x_col='path', y_col='class',\n",
    "                                              image_size=(img_hight, img_width), target_size=(\n",
    "                                                  img_hight, img_hight), color_mode='rgb',\n",
    "                                              batch_size=batch_size, class_mode='categorical',\n",
    "                                              classes=clases, subset=None, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce1ca",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344814fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(Image_shape, block1=True, block2=True, block3=True,\n",
    "                 block4=True, block5=True, lstm=True, regularizer=keras.regularizers.l2(0.0001),\n",
    "                 Dropout_ratio=0.15):\n",
    "\n",
    "    # * Create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # * configure the inputshape\n",
    "    model.add(keras.Input(shape=Image_shape))\n",
    "\n",
    "    # * Add the first block\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the second block\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block2, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block2, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the third block\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the fourth block\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the fifth block\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add((MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "    # * Reshape the output of the last layer to be used in the GRU layer\n",
    "    model.add(keras.layers.Reshape((7*7, 512)))\n",
    "    model.add(GRU(512, activation='relu', trainable=lstm, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #* flatten + Fc layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(Dropout_ratio))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # * Output layer\n",
    "    #model.add(Dense(3, activation='linear'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "#* compile function\n",
    "def model_compiling(model, loss = 'categorical_crossentropy', optimizer = 'adam'):\n",
    "    model.compile(\n",
    "        #loss =keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_2(Image_shape, block1=True, block2=True, block3=True,\n",
    "                 block4=True, block5=True, lstm=True, regularizer=keras.regularizers.l2(0.0001),\n",
    "                 Dropout_ratio=0.15):\n",
    "\n",
    "    # * Create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # * configure the inputshape\n",
    "    model.add(keras.Input(shape=Image_shape))\n",
    "\n",
    "    # * Add the first block\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the second block\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the third block\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block4, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # * Add the fourth block\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block5, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add((MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "    # * Reshape the output of the last layer to be used in the GRU layer\n",
    "    model.add(keras.layers.Reshape((7*7, 512)))\n",
    "    model.add(GRU(512, activation='relu', trainable=lstm, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #* flatten + Fc layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(Dropout_ratio))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # * Output layer\n",
    "    #model.add(Dense(3, activation='linear'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec489156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_4(Image_shape, block1=True, block2=True, block3=True,\n",
    "                 block4=True, block5=True, lstm=True, regularizer=keras.regularizers.l2(0.0001),\n",
    "                 Dropout_ratio=0.15):\n",
    "\n",
    "    # * Create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # * configure the inputshape\n",
    "    model.add(keras.Input(shape=Image_shape))\n",
    "\n",
    "    # * Add the first block\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    # * Add the second block\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #Adding additional blocks\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(1024, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(1024, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     # * Add the third block\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     # * Add the fourth block\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "# #     model.add((MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "    # * Reshape the output of the last layer to be used in the GRU layer\n",
    "    model.add(keras.layers.Reshape((7*7, 1024)))\n",
    "    model.add(GRU(512, activation='relu', trainable=lstm, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #* flatten + Fc layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(Dropout_ratio))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # * Output layer\n",
    "    #model.add(Dense(3, activation='linear'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model_3(Image_shape, block1=True, block2=True, block3=True,\n",
    "                 block4=True, block5=True, lstm=True, regularizer=keras.regularizers.l2(0.0001),\n",
    "                 Dropout_ratio=0.15):\n",
    "\n",
    "    # * Create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # * configure the inputshape\n",
    "    model.add(keras.Input(shape=Image_shape))\n",
    "\n",
    "    # * Add the first block\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block1, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.3))\n",
    "\n",
    "    # * Add the second block\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block2, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block2, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.3))\n",
    "\n",
    "    # * Add the third block\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu',\n",
    "              trainable=block3, kernel_regularizer=regularizer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.3))\n",
    "\n",
    "#     # * Add the fourth block\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block4, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     # model.add(Dropout(0.3))\n",
    "\n",
    "#     # * Add the fifth block\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(Conv2D(512, (3, 3), padding='same', activation='relu',\n",
    "#               trainable=block5, kernel_regularizer=regularizer))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add((MaxPooling2D(pool_size=(2, 2))))\n",
    "#     # model.add(Dropout(0.3))\n",
    "\n",
    "    # * Reshape the output of the last layer to be used in the GRU layer\n",
    "    model.add(keras.layers.Reshape((28*28, 256)))\n",
    "    model.add(GRU(256, activation='relu', trainable=lstm, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #* flatten + Fc layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(Dropout_ratio))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # * Output layer\n",
    "    #model.add(Dense(3, activation='linear'))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c53f9c",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Model_2_Acc_based.h5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                            filepath=model_path,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_accuracy',\n",
    "                            save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=6, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)\n",
    "model_path_2 = \"Model_2_Acc_based_2.h5\"\n",
    "checkpoint_2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                            filepath=model_path_2,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_accuracy',\n",
    "                            save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=6, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)\n",
    "model_path_3 = \"Model_2_Acc_based_3.h5\"\n",
    "checkpoint_3 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                            filepath=model_path_3,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_accuracy',\n",
    "                            save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=6, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)\n",
    "model_path_4 = \"Model_2_Acc_based_4.h5\"\n",
    "checkpoint_4 = tf.keras.callbacks.ModelCheckpoint(\n",
    "                            filepath=model_path_4,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_accuracy',\n",
    "                            save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=6, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3b60e",
   "metadata": {},
   "source": [
    "# Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24619f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Create_model(size)\n",
    "model_compiling(model1, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model2 = Create_model_2(size)\n",
    "model_compiling(model2, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model3 = Create_model_3(size)\n",
    "model_compiling(model3, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model4 = Create_model_4(size)\n",
    "model_compiling(model4, loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a5742",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model1.fit(\n",
    "train_data, \n",
    "validation_data= val_data, \n",
    "epochs=120, \n",
    "callbacks=[earlystop, checkpoint, learning_rate_reduction])\n",
    "history2 = model2.fit(\n",
    "train_data, \n",
    "validation_data= val_data, \n",
    "epochs=120, \n",
    "callbacks=[earlystop, checkpoint_2, learning_rate_reduction])\n",
    "history3 = model3.fit(\n",
    "train_data, \n",
    "validation_data= val_data, \n",
    "epochs=120, \n",
    "callbacks=[earlystop, checkpoint_3, learning_rate_reduction])\n",
    "history4 = model4.fit(\n",
    "train_data, \n",
    "validation_data= val_data, \n",
    "epochs=120, \n",
    "callbacks=[earlystop, checkpoint_4, learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784cd838",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history1.history)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(history1.history), 'history1_acc_monitoring.csv', index=False)\n",
    "pd.DataFrame(history2.history)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(history2.history), 'history2_acc_monitoring.csv', index=False)\n",
    "pd.DataFrame(history3.history)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(history3.history), 'history3_acc_monitoring.csv', index=False)\n",
    "pd.DataFrame(history4.history)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(history4.history), 'history4_acc_monitoring.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff4541",
   "metadata": {},
   "source": [
    "# Retrieving the model and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "# #* Download the model weights and history\n",
    "url = \"https://drive.google.com/file/d/1MOBkvpaCEg6bJ_3MPKO7j8wFOHCyrNHl/view?usp=share_link\"\n",
    "output = \"model.h5\"\n",
    "gdown.download(url=url, output=output, quiet=False, fuzzy=True)\n",
    "\n",
    "url = \"https://drive.google.com/file/d/1B10MoA1PTb8FtCQzTxlKFwhK4iBAtTT7/view?usp=share_link\"\n",
    "output = \"Model_history.csv\"\n",
    "gdown.download(url=url, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328cdc3",
   "metadata": {},
   "source": [
    "# Recreating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3734f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Create_model(size)\n",
    "model_compiling(model1, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model2 = Create_model_2(size)\n",
    "model_compiling(model2, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model3 = Create_model_3(size)\n",
    "model_compiling(model3, loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "model4 = Create_model_4(size)\n",
    "model_compiling(model4, loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ac517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights('./Model_2_Acc_based.h5')\n",
    "model2.load_weights('./Model_2_Acc_based_2.h5')\n",
    "model3.load_weights('./Model_2_Acc_based_3.h5')\n",
    "model4.load_weights('./Model_2_Acc_based_4.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b7ffe",
   "metadata": {},
   "source": [
    "# Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ded5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(test_data)\n",
    "pred2 = model2.predict(test_data)\n",
    "pred3 = model3.predict(test_data)\n",
    "pred4 = model4.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data_unshuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions of the model on the unshuffled test set\n",
    "predictions = model.predict(test_data_unshuffled, verbose=1 ,\n",
    "                            workers=5, use_multiprocessing=True)\n",
    "predictions.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34044c20",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_1 = test_data.class_indices\n",
    "class_dict_1 = {value: key for key, value in class_dict.items()} \n",
    "predicted_classes_1 = [class_dict.get(list(pred1[i]).index(pred1[i].max())) for i in range(len(pred1))]\n",
    "Test_data['predicted_class_1'] = predicted_classes_1\n",
    "Test_data.sample(15)\n",
    "\n",
    "class_dict_2 = test_data.class_indices\n",
    "class_dict_2 = {value: key for key, value in class_dict.items()} \n",
    "predicted_classes_2 = [class_dict.get(list(pred2[i]).index(pred2[i].max())) for i in range(len(pred2))]\n",
    "Test_data['predicted_class_2'] = predicted_classes_2\n",
    "Test_data.sample(15)\n",
    "\n",
    "class_dict_3 = test_data.class_indices\n",
    "class_dict_3 = {value: key for key, value in class_dict.items()} \n",
    "predicted_classes_3 = [class_dict.get(list(pred3[i]).index(pred3[i].max())) for i in range(len(pred3))]\n",
    "Test_data['predicted_class_3'] = predicted_classes_3\n",
    "Test_data.sample(15)\n",
    "\n",
    "class_dict_4 = test_data.class_indices\n",
    "class_dict_4 = {value: key for key, value in class_dict.items()} \n",
    "predicted_classes_4 = [class_dict.get(list(pred4[i]).index(pred4[i].max())) for i in range(len(pred4))]\n",
    "Test_data['predicted_class_4'] = predicted_classes_4\n",
    "Test_data.sample(15)\n",
    "Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ed83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Data = pd.DataFrame()\n",
    "S_Data['pm_1'] = Test_data['predicted_class_1']\n",
    "S_Data['pm_2'] = Test_data['predicted_class_2']\n",
    "S_Data['pm_3'] = Test_data['predicted_class_3']\n",
    "S_Data['pm_4'] = Test_data['predicted_class_4']\n",
    "S_Data['target'] = Test_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ef360",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(S_Data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Data = S_Data.replace('COVID19', 0)\n",
    "S_Data = S_Data.replace('NORMAL', 1)\n",
    "S_Data = S_Data.replace('PNEUMONIA', 2)\n",
    "display(S_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c354246",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = S_Data.iloc[:, :-1]\n",
    " y = S_Data.iloc[:, -1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.1,random_state =123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac1f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0bb88",
   "metadata": {},
   "source": [
    "# Confirming the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct prediction: \" ,len(Test_data[Test_data['matched' ]== True]))\n",
    "print(\"False prediction: \" ,len(Test_data[Test_data['matched' ]== False]))\n",
    "print(\"Accucary of the model on unseen data \" , round(len(Test_data[Test_data['matched' ]== True])/len(Test_data),4), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75899203",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b42c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "conf_mat = confusion_matrix(Test_data['class'], \n",
    "                             Test_data['predicted_class'],\n",
    "                             labels = list(class_dict.values()))\n",
    "\n",
    " def confusion_matrix_plot(conf_mat,labels = list(class_dict.values()),length = len(Test_data)):\n",
    "     fig = px.imshow(conf_mat,\n",
    "                     labels=dict(x=\"Actual-class\", y=\"Predicted-class\"), color_continuous_scale='viridis',\n",
    "                     x=labels,\n",
    "                     y=labels,\n",
    "                     text_auto=True, aspect=\"auto\", range_color = [0,length]\n",
    "                    )\n",
    "     fig.update_xaxes(side=\"top\")\n",
    "     fig.update_layout(font=dict(color='black'))\n",
    "     fig.show()\n",
    "confusion_matrix_plot(conf_mat,labels = list(class_dict.values()),length = len(Test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(conf_mat, annot=True, linewidth=.5, cmap=sns.cubehelix_palette(as_cmap=True), fmt='.0f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
